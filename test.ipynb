{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, this is an interesting project involving web scraping, data processing (NLP/Geocoding), clustering, and serving data via an API. As a software engineer with web development experience, I'll structure this backend logically.\n",
    "\n",
    "Here's my thought process:\n",
    "\n",
    "Understand the Goal: The core task is to get Singapore news, find locations mentioned, group news by location, and provide this data via an API for a map frontend.\n",
    "\n",
    "Break Down the Backend:\n",
    "\n",
    "Web Scraping: Need to fetch news articles from reliable Singapore news sources. This requires HTTP requests and HTML parsing. Must be mindful of website terms of service and robots.txt.\n",
    "\n",
    "Content Extraction: From the scraped pages, extract relevant info: title, URL, summary/content snippet, possibly publication date.\n",
    "\n",
    "Location Identification (NER): Scan the extracted text to identify mentions of locations within Singapore. This is a Natural Language Processing (NLP) task. A simple keyword list or a more sophisticated NER library could be used.\n",
    "\n",
    "Geocoding: Convert identified location names (e.g., \"Orchard Road\", \"Changi Airport\", \"Bishan\") into geographical coordinates (latitude, longitude). This usually requires an external geocoding service.\n",
    "\n",
    "Clustering/Grouping: Group articles based on their associated coordinates. Since the goal is to show articles when hovering over a region on the map, we might group by predefined planning areas/districts or use a spatial clustering algorithm (like DBSCAN or K-Means) on the coordinates. Simple grouping by the primary identified location might be a good starting point.\n",
    "\n",
    "Data Storage (Optional but Recommended): Storing scraped and processed data can prevent re-scraping and re-processing constantly. A simple database (like SQLite) or even caching could be used. For this example, we might keep it in memory for simplicity, but acknowledge this limitation.\n",
    "\n",
    "API Endpoint: Create a web server (using Flask or FastAPI) to expose an endpoint (e.g., /api/news/clusters) that the frontend can call to get the clustered news data.\n",
    "\n",
    "Choose Tools/Libraries:\n",
    "\n",
    "Web Framework: Flask (simple, good for APIs) or FastAPI (modern, fast, good type hinting). Let's use Flask for this example due to its simplicity.\n",
    "\n",
    "HTTP Requests: requests library.\n",
    "\n",
    "HTML Parsing: BeautifulSoup4 library.\n",
    "\n",
    "Location Identification:\n",
    "\n",
    "Simple: Predefined list of Singapore locations + string matching.\n",
    "\n",
    "Advanced: spaCy or NLTK for Named Entity Recognition (NER). Let's start with a simplified keyword approach for demonstration, but mention spaCy as a better alternative.\n",
    "\n",
    "Geocoding: geopy library (interfaces with services like Nominatim/OpenStreetMap). Requires careful handling of usage limits and terms of service for the chosen provider.\n",
    "\n",
    "Clustering (Optional Advanced): scikit-learn if using spatial clustering algorithms. For now, we'll group by geocoded location name.\n",
    "\n",
    "Concurrency (Optional Advanced): asyncio or background task queues (Celery, RQ) would be needed for non-blocking scraping in a real application.\n",
    "\n",
    "Structure the Code:\n",
    "\n",
    "app.py: Main Flask application, defines API routes.\n",
    "\n",
    "scraper.py: Functions for fetching and parsing news articles.\n",
    "\n",
    "processing.py: Functions for location extraction, geocoding, and grouping/clustering.\n",
    "\n",
    "config.py: Configuration like news sources, location keywords.\n",
    "\n",
    "requirements.txt: List of dependencies.\n",
    "\n",
    "Address Potential Issues:\n",
    "\n",
    "Scraping Ethics/Legality: Always check robots.txt and Terms of Service. Websites change structure, breaking scrapers. Use responsibly.\n",
    "\n",
    "Geocoding Limits: Free services like Nominatim have rate limits. Caching is essential.\n",
    "\n",
    "Location Ambiguity: Simple keyword matching might find \"Springfield\" which isn't in Singapore. Geocoding queries should specify \"Singapore\". NER is generally better at context.\n",
    "\n",
    "Performance: Scraping and geocoding can be slow. Real apps need background tasks and caching/database storage.\n",
    "\n",
    "Clustering Accuracy: Simple grouping might place distinct nearby events in the same bucket if they share a common location name. Spatial clustering offers more geographic precision but requires tuning.\n",
    "\n",
    "Let's start building the components.\n",
    "\n",
    "1. Project Setup & Dependencies\n",
    "\n",
    "First, create a project directory and define the necessary libraries in requirements.txt:\n",
    "\n",
    "# requirements.txt\n",
    "flask\n",
    "requests\n",
    "beautifulsoup4\n",
    "geopy\n",
    "Use code with caution.\n",
    "Text\n",
    "Install them: pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Configuration (config.py)\n",
    "\n",
    "Define news sources and known Singapore locations. Note: CSS selectors are highly specific to the website's structure at a given time and will likely need updating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config.py\n",
    "\n",
    "# --- News Sources Configuration ---\n",
    "# IMPORTANT: Check robots.txt and Terms of Service before scraping!\n",
    "# CSS Selectors are examples and WILL need verification and updates.\n",
    "NEWS_SOURCES = [\n",
    "    {\n",
    "        'name': 'Channel News Asia (Singapore)',\n",
    "        'url': 'https://www.channelnewsasia.com/singapore',\n",
    "        # Example selectors (likely need adjustment)\n",
    "        'selectors': {\n",
    "            'article_container': 'div.list-object', # Find a container for each article\n",
    "            'title': 'h6.list-object__headline a', # Selector for the title text/link\n",
    "            'link': 'h6.list-object__headline a', # Selector for the link href\n",
    "            'summary': 'p.list-object__description', # Selector for the summary\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'name': 'Straits Times (Singapore)',\n",
    "        'url': 'https://www.straitstimes.com/singapore',\n",
    "         # Example selectors (likely need adjustment)\n",
    "         # ST often uses dynamic class names, making scraping harder.\n",
    "         # Might require browser automation (Selenium) or inspecting network requests.\n",
    "        'selectors': {\n",
    "            'article_container': 'div.card', # Highly speculative selector\n",
    "            'title': 'h5.card-title a',\n",
    "            'link': 'h5.card-title a',\n",
    "            'summary': 'p.card-text',\n",
    "        }\n",
    "    },\n",
    "    # Add more sources if desired\n",
    "]\n",
    "\n",
    "# --- Location Configuration ---\n",
    "# A list of known Singapore locations for simple keyword matching.\n",
    "# More comprehensive lists exist (e.g., Planning Areas, MRT stations).\n",
    "# Consider using a more structured approach (e.g., dictionary with aliases)\n",
    "# or an NLP library for better accuracy.\n",
    "SINGAPORE_LOCATIONS = [\n",
    "    \"Orchard Road\", \"Marina Bay\", \"Sentosa\", \"Changi Airport\", \"Jurong East\",\n",
    "    \"Jurong West\", \"Tampines\", \"Pasir Ris\", \"Woodlands\", \"Yishun\", \"Ang Mo Kio\",\n",
    "    \"Bishan\", \"Toa Payoh\", \"Bukit Merah\", \"Queenstown\", \"Clementi\", \"Bukit Timah\",\n",
    "    \"Novena\", \"Geylang\", \"Bedok\", \"Punggol\", \"Sengkang\", \"Hougang\", \"Serangoon\",\n",
    "    \"Bukit Panjang\", \"Choa Chu Kang\", \"Tuas\", \"Pulau Ubin\", \"Tekong\",\n",
    "    # Add more specific landmarks, streets, MRT stations etc.\n",
    "    \"Raffles Place\", \"Tanjong Pagar\", \"City Hall\", \"Dhoby Ghaut\", \"Somerset\",\n",
    "    \"Newton\", \"Stevens\", \"Botanic Gardens\", \"Holland Village\", \"Buona Vista\",\n",
    "    \"Commonwealth\", \"Dover\", \"Outram Park\", \"HarbourFront\", \"Telok Blangah\",\n",
    "    \"Labrador Park\", \"Pasir Panjang\", \"Haw Par Villa\", \"Kent Ridge\", \"one-north\",\n",
    "]\n",
    "\n",
    "# --- Geocoding Configuration ---\n",
    "GEOCODER_USER_AGENT = \"singapore_news_mapper_app_v0.1\" # Be descriptive and unique\n",
    "GEOCODING_CACHE = {} # Simple in-memory cache for geocoding results\n",
    "\n",
    "# --- API Configuration ---\n",
    "API_HOST = '0.0.0.0'\n",
    "API_PORT = 5000\n",
    "API_DEBUG = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraper.py\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import logging\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def fetch_html(url: str) -> Optional[str]:\n",
    "    \"\"\"Fetches HTML content from a given URL.\"\"\"\n",
    "    try:\n",
    "        headers = { # Mimic a browser to avoid simple blocks\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        response = requests.get(url, headers=headers, timeout=15) # Added timeout\n",
    "        response.raise_for_status() # Raise an exception for bad status codes (4xx or 5xx)\n",
    "        return response.text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.error(f\"Error fetching URL {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def parse_articles_from_html(html_content: str, config: Dict) -> List[Dict]:\n",
    "    \"\"\"Parses articles from HTML content based on provided CSS selectors.\"\"\"\n",
    "    articles = []\n",
    "    if not html_content:\n",
    "        return articles\n",
    "\n",
    "    try:\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        selectors = config['selectors']\n",
    "        base_url = config.get('base_url', '') # Optional base URL if links are relative\n",
    "\n",
    "        # Find all article containers\n",
    "        article_elements = soup.select(selectors['article_container'])\n",
    "        logging.info(f\"Found {len(article_elements)} potential article elements using selector '{selectors['article_container']}' for {config['name']}\")\n",
    "\n",
    "        for element in article_elements:\n",
    "            title_element = element.select_one(selectors['title'])\n",
    "            link_element = element.select_one(selectors['link'])\n",
    "            summary_element = element.select_one(selectors['summary'])\n",
    "\n",
    "            title = title_element.get_text(strip=True) if title_element else None\n",
    "            raw_link = link_element['href'] if link_element and link_element.has_attr('href') else None\n",
    "            summary = summary_element.get_text(strip=True) if summary_element else \"\" # Use empty string if no summary\n",
    "\n",
    "            if title and raw_link:\n",
    "                # Construct absolute URL if necessary\n",
    "                link = requests.compat.urljoin(config['url'], raw_link) if base_url or not raw_link.startswith('http') else raw_link\n",
    "\n",
    "                articles.append({\n",
    "                    'title': title,\n",
    "                    'url': link,\n",
    "                    'summary': summary,\n",
    "                    'source': config['name']\n",
    "                })\n",
    "            else:\n",
    "                 logging.warning(f\"Skipping element, missing title or link. Element: {str(element)[:100]}...\")\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error parsing HTML for {config['name']}: {e}\")\n",
    "\n",
    "    return articles\n",
    "\n",
    "# --- Main Scraping Function ---\n",
    "\n",
    "def scrape_news_sources(sources_config: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"Scrapes news articles from a list of configured sources.\"\"\"\n",
    "    all_articles = []\n",
    "    logging.info(f\"Starting scraping process for {len(sources_config)} sources...\")\n",
    "\n",
    "    for source in sources_config:\n",
    "        logging.info(f\"Scraping source: {source['name']} ({source['url']})\")\n",
    "        html = fetch_html(source['url'])\n",
    "        if html:\n",
    "            parsed_articles = parse_articles_from_html(html, source)\n",
    "            logging.info(f\"Successfully parsed {len(parsed_articles)} articles from {source['name']}\")\n",
    "            all_articles.extend(parsed_articles)\n",
    "        else:\n",
    "            logging.warning(f\"Could not fetch or parse content for {source['name']}\")\n",
    "\n",
    "    logging.info(f\"Scraping finished. Total articles collected: {len(all_articles)}\")\n",
    "    return all_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'config'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SINGAPORE_LOCATIONS, GEOCODER_USER_AGENT, GEOCODING_CACHE\n\u001b[0;32m     12\u001b[0m logging\u001b[38;5;241m.\u001b[39mbasicConfig(level\u001b[38;5;241m=\u001b[39mlogging\u001b[38;5;241m.\u001b[39mINFO, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%(asctime)s\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m%(levelname)s\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m%(message)s\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# --- Location Extraction ---\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'config'"
     ]
    }
   ],
   "source": [
    "# processing.py\n",
    "\n",
    "import logging\n",
    "from typing import List, Dict, Optional, Tuple, Set\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.exc import GeocoderTimedOut, GeocoderServiceError\n",
    "import time\n",
    "import re\n",
    "\n",
    "from config import SINGAPORE_LOCATIONS, GEOCODER_USER_AGENT, GEOCODING_CACHE\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# --- Location Extraction ---\n",
    "\n",
    "def extract_locations_from_text(text: str, known_locations: List[str]) -> Set[str]:\n",
    "    \"\"\"\n",
    "    Finds mentions of known locations within a given text using simple keyword matching.\n",
    "    Returns a set of unique locations found.\n",
    "    \"\"\"\n",
    "    found_locations = set()\n",
    "    # Use word boundaries to avoid partial matches (e.g., \"Bedok\" in \"BedokReservoired\")\n",
    "    # Case-insensitive matching\n",
    "    for loc in known_locations:\n",
    "        # Simple boundary check: space, punctuation, start/end of string\n",
    "        pattern = r'(?i)(?<!\\w)' + re.escape(loc) + r'(?!\\w)'\n",
    "        if re.search(pattern, text):\n",
    "            found_locations.add(loc)\n",
    "    return found_locations\n",
    "\n",
    "# --- Geocoding ---\n",
    "\n",
    "def get_geocoder() -> Nominatim:\n",
    "    \"\"\"Initializes and returns a Nominatim geocoder instance.\"\"\"\n",
    "    return Nominatim(user_agent=GEOCODER_USER_AGENT)\n",
    "\n",
    "def geocode_location(location_name: str, geolocator: Nominatim, attempt=1, max_attempts=3) -> Optional[Tuple[float, float]]:\n",
    "    \"\"\"\n",
    "    Geocodes a location name to (latitude, longitude).\n",
    "    Uses a simple in-memory cache. Appends 'Singapore' for better results.\n",
    "    Includes basic retry logic for timeouts.\n",
    "    \"\"\"\n",
    "    cache_key = location_name.lower()\n",
    "    if cache_key in GEOCODING_CACHE:\n",
    "        logging.debug(f\"Cache hit for geocoding: {location_name}\")\n",
    "        return GEOCODING_CACHE[cache_key]\n",
    "\n",
    "    query = f\"{location_name}, Singapore\"\n",
    "    logging.debug(f\"Geocoding query: '{query}' (Attempt {attempt})\")\n",
    "\n",
    "    try:\n",
    "        # Add a small delay to respect usage policies\n",
    "        time.sleep(1)\n",
    "        location_data = geolocator.geocode(query, exactly_one=True, timeout=10) # Added timeout\n",
    "\n",
    "        if location_data:\n",
    "            coords = (location_data.latitude, location_data.longitude)\n",
    "            GEOCODING_CACHE[cache_key] = coords # Cache the result\n",
    "            logging.debug(f\"Geocoded '{location_name}' to {coords}\")\n",
    "            return coords\n",
    "        else:\n",
    "            logging.warning(f\"Could not geocode location: {location_name}\")\n",
    "            GEOCODING_CACHE[cache_key] = None # Cache failure to avoid retrying invalid locations\n",
    "            return None\n",
    "\n",
    "    except GeocoderTimedOut:\n",
    "        logging.warning(f\"Geocoder timed out for: {location_name}. Retrying if possible...\")\n",
    "        if attempt < max_attempts:\n",
    "            time.sleep(attempt * 2) # Exponential backoff\n",
    "            return geocode_location(location_name, geolocator, attempt + 1, max_attempts)\n",
    "        else:\n",
    "            logging.error(f\"Geocoder timed out after {max_attempts} attempts for: {location_name}\")\n",
    "            GEOCODING_CACHE[cache_key] = None\n",
    "            return None\n",
    "    except GeocoderServiceError as e:\n",
    "        logging.error(f\"Geocoder service error for {location_name}: {e}\")\n",
    "        GEOCODING_CACHE[cache_key] = None\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unexpected error during geocoding for {location_name}: {e}\")\n",
    "        GEOCODING_CACHE[cache_key] = None\n",
    "        return None\n",
    "\n",
    "\n",
    "# --- Article Processing and Grouping ---\n",
    "\n",
    "def process_and_group_articles(articles: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Processes articles to find locations, geocode them, and group them by location.\n",
    "    Returns a list of clusters, where each cluster has location info and associated articles.\n",
    "    \"\"\"\n",
    "    geolocator = get_geocoder()\n",
    "    articles_with_location = []\n",
    "\n",
    "    logging.info(f\"Processing {len(articles)} articles for locations...\")\n",
    "\n",
    "    # 1. Find locations and geocode the *first* valid one found for simplicity\n",
    "    for article in articles:\n",
    "        text_to_scan = f\"{article['title']} {article['summary']}\"\n",
    "        found_locations = extract_locations_from_text(text_to_scan, SINGAPORE_LOCATIONS)\n",
    "\n",
    "        primary_location_name = None\n",
    "        coordinates = None\n",
    "\n",
    "        if found_locations:\n",
    "            # Try geocoding found locations until one succeeds\n",
    "            for loc in found_locations:\n",
    "                coords = geocode_location(loc, geolocator)\n",
    "                if coords:\n",
    "                    primary_location_name = loc # Use the name that was successfully geocoded\n",
    "                    coordinates = coords\n",
    "                    logging.debug(f\"Article '{article['title'][:30]}...' associated with location '{primary_location_name}' {coordinates}\")\n",
    "                    break # Use the first successfully geocoded location\n",
    "            if coordinates:\n",
    "                 articles_with_location.append({**article, 'location_name': primary_location_name, 'coords': coordinates})\n",
    "            else:\n",
    "                 logging.debug(f\"Article '{article['title'][:30]}...' had locations {found_locations} but none could be geocoded.\")\n",
    "        else:\n",
    "             logging.debug(f\"No known locations found in article '{article['title'][:30]}...'\")\n",
    "\n",
    "\n",
    "    logging.info(f\"Found locations and geocoded for {len(articles_with_location)} articles.\")\n",
    "\n",
    "    # 2. Group articles by coordinates (simple grouping)\n",
    "    # We use coordinates as the key for grouping to handle cases where different names might geocode to the same spot.\n",
    "    # Using a tuple of floats as dict key might have precision issues, converting to string is safer.\n",
    "    grouped_by_coords = {}\n",
    "    for article in articles_with_location:\n",
    "        coord_key = f\"{article['coords'][0]:.5f},{article['coords'][1]:.5f}\" # Key based on rounded coords\n",
    "        if coord_key not in grouped_by_coords:\n",
    "            grouped_by_coords[coord_key] = {\n",
    "                'latitude': article['coords'][0],\n",
    "                'longitude': article['coords'][1],\n",
    "                # Store the first location name associated with these coords\n",
    "                # A better approach might list all names or use a canonical name\n",
    "                'location_name': article['location_name'],\n",
    "                'articles': []\n",
    "            }\n",
    "        # Ensure we don't add duplicate articles if scraped from multiple sources covering the same event\n",
    "        # Basic check based on URL\n",
    "        if not any(a['url'] == article['url'] for a in grouped_by_coords[coord_key]['articles']):\n",
    "             grouped_by_coords[coord_key]['articles'].append({\n",
    "                 'title': article['title'],\n",
    "                 'url': article['url'],\n",
    "                 'summary': article['summary'],\n",
    "                 'source': article['source']\n",
    "             })\n",
    "\n",
    "\n",
    "    # 3. Format the output list\n",
    "    clusters = []\n",
    "    for data in grouped_by_coords.values():\n",
    "        clusters.append({\n",
    "            'latitude': data['latitude'],\n",
    "            'longitude': data['longitude'],\n",
    "            'location_name': data['location_name'], # Display name\n",
    "            'article_count': len(data['articles']),\n",
    "            'articles': data['articles']\n",
    "        })\n",
    "\n",
    "    logging.info(f\"Grouped articles into {len(clusters)} location clusters.\")\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# app.py\n",
    "\n",
    "from flask import Flask, jsonify, request\n",
    "import logging\n",
    "\n",
    "from config import NEWS_SOURCES, API_HOST, API_PORT, API_DEBUG\n",
    "from scraper import scrape_news_sources\n",
    "from processing import process_and_group_articles\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# --- In-memory Cache for API results (Simple Caching) ---\n",
    "# In a real app, use Redis, Memcached, or a proper caching library/database.\n",
    "# Also, implement cache invalidation (e.g., time-based).\n",
    "API_CACHE = {\n",
    "    'clustered_news': None,\n",
    "    'last_updated': None\n",
    "}\n",
    "CACHE_TTL_SECONDS = 60 * 30 # Cache results for 30 minutes\n",
    "\n",
    "# --- API Endpoint ---\n",
    "\n",
    "@app.route('/api/news/clusters', methods=['GET'])\n",
    "def get_news_clusters():\n",
    "    \"\"\"\n",
    "    API endpoint to retrieve news articles clustered by location.\n",
    "    Uses a simple time-based cache.\n",
    "    \"\"\"\n",
    "    import time\n",
    "    now = time.time()\n",
    "\n",
    "    # Check cache\n",
    "    if API_CACHE['clustered_news'] and API_CACHE['last_updated'] and (now - API_CACHE['last_updated'] < CACHE_TTL_SECONDS):\n",
    "        logging.info(\"Serving clustered news data from cache.\")\n",
    "        return jsonify(API_CACHE['clustered_news'])\n",
    "\n",
    "    logging.info(\"Cache miss or expired. Fetching and processing fresh news data...\")\n",
    "    try:\n",
    "        # 1. Scrape Data\n",
    "        articles = scrape_news_sources(NEWS_SOURCES)\n",
    "        if not articles:\n",
    "             # Return potentially stale cache data if scraping fails, or an error\n",
    "             if API_CACHE['clustered_news']:\n",
    "                 logging.warning(\"Scraping failed, returning stale cache data.\")\n",
    "                 return jsonify(API_CACHE['clustered_news'])\n",
    "             else:\n",
    "                 return jsonify({\"error\": \"Failed to scrape news sources and no cache available.\"}), 500\n",
    "\n",
    "\n",
    "        # 2. Process and Cluster Data\n",
    "        clustered_data = process_and_group_articles(articles)\n",
    "\n",
    "        # 3. Update Cache\n",
    "        API_CACHE['clustered_news'] = clustered_data\n",
    "        API_CACHE['last_updated'] = now\n",
    "        logging.info(\"Successfully updated API cache with fresh data.\")\n",
    "\n",
    "        # 4. Return Data\n",
    "        return jsonify(clustered_data)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.exception(\"An error occurred while processing the request.\")\n",
    "        # Return potentially stale cache data on error, or a generic error\n",
    "        if API_CACHE['clustered_news']:\n",
    "             logging.warning(\"Processing failed, returning stale cache data.\")\n",
    "             return jsonify(API_CACHE['clustered_news'])\n",
    "        else:\n",
    "            return jsonify({\"error\": \"An internal server error occurred.\"}), 500\n",
    "\n",
    "# --- Basic Health Check Endpoint ---\n",
    "@app.route('/health', methods=['GET'])\n",
    "def health_check():\n",
    "    \"\"\"A simple health check endpoint.\"\"\"\n",
    "    return jsonify({\"status\": \"ok\"}), 200\n",
    "\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == '__main__':\n",
    "    logging.info(f\"Starting Flask server on {API_HOST}:{API_PORT}\")\n",
    "    # Use waitress or gunicorn for production instead of Flask's development server\n",
    "    app.run(host=API_HOST, port=API_PORT, debug=API_DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to Run:\n",
    "\n",
    "Save the files as config.py, scraper.py, processing.py, app.py, and requirements.txt.\n",
    "\n",
    "Install dependencies: pip install -r requirements.txt\n",
    "\n",
    "Run the Flask app: python app.py\n",
    "\n",
    "Access the API endpoint in your browser or using curl: http://127.0.0.1:5000/api/news/clusters (or http://localhost:5000/api/news/clusters)\n",
    "\n",
    "Next Steps & Improvements:\n",
    "\n",
    "Frontend Development: Build the HTML, CSS, and JavaScript frontend using a mapping library (like Leaflet, Mapbox GL JS, or Google Maps API). The JavaScript code will fetch data from /api/news/clusters, iterate through the results, place markers/clusters on the map, and attach popups that display the articles list on hover/click.\n",
    "\n",
    "Refine Scraping: The CSS selectors in config.py are placeholders and will need to be carefully inspected and updated for the target websites. Scraping is fragile. Consider using browser automation tools like Selenium or Playwright if sites heavily rely on JavaScript rendering, but this adds complexity. Respect robots.txt and Terms of Service.\n",
    "\n",
    "Improve Location Extraction: Replace the simple keyword list with a more robust NLP approach using libraries like spaCy. Train or fine-tune a model for Singapore locations for better accuracy.\n",
    "\n",
    "Enhance Geocoding: Implement more robust error handling and potentially use multiple geocoding providers as fallbacks. Persist the geocoding cache (e.g., in a file or database) instead of just in memory.\n",
    "\n",
    "Background Tasks: Move scraping and processing to background tasks (using Celery, RQ, or apscheduler) so the API endpoint responds quickly by serving cached/pre-processed data, rather than performing slow operations on demand.\n",
    "\n",
    "Database Storage: Store scraped articles, locations, and geocoding results in a database (like PostgreSQL with PostGIS for spatial queries, or even SQLite for simplicity) for persistence, better querying, and scalability.\n",
    "\n",
    "Clustering Algorithm: Implement spatial clustering (e.g., DBSCAN from scikit-learn) on the coordinates for more geographically meaningful clusters, especially if many articles mention nearby but distinct locations.\n",
    "\n",
    "Error Handling & Logging: Add more comprehensive error handling and logging throughout the application.\n",
    "\n",
    "Deployment: For production, use a proper WSGI server (like Gunicorn or uWSGI) behind a reverse proxy (like Nginx).\n",
    "\n",
    "This provides a solid, well-structured backend foundation for your Singapore news map application. Remember that the scraping and location extraction parts often require the most ongoing maintenance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scrape_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
